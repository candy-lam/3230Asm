{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# pip install torch torchvision numpy matplotlib\nimport torch\nimport torchvision\nfrom torch import nn\nimport torch.nn.functional as func\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"id":"SkV6khCfF8gb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainLoss_plot = []\ntestLoss_plot = []\ntrainAcc_plot = []\ntestAcc_plot = []","metadata":{"id":"9eoltSduSGXn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train transformation\ntrain_transform = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5), (0.5))\n])\n# Test transformation\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.45), (0.5))\n])\n\n# Download training data from open datasets.\ntraining_data = datasets.CIFAR10(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=train_transform,\n)\n\n# Download test data from open datasets.\ntest_data = datasets.CIFAR10(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=test_transform,\n)","metadata":{"id":"L6SSr2ObF8gi","outputId":"ca63c7c4-7f69-438a-beed-b1a3360429cd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training batch size\nbatch_size = 256\n\n# Create data loaders.\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.figure(figsize=(15,15))\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# Each batch tensor shape\nfor X, y in test_dataloader:\n    print(\"Shape of X [N, C, H, W]: \", X.shape)\n    print(\"Shape of y: \", y.shape, y.dtype)\n    \n    imshow(torchvision.utils.make_grid(X, nrow=20))\n    \n    break","metadata":{"id":"8xidh-FiF8gm","outputId":"332a1fb9-2bba-4dbb-dcf7-32ee41c41c31"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get cpu or gpu device for training.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using {} device\".format(device))\n\n# Define model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        '''\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(3 * 32 * 32, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n        '''\n        self.Conv2d_stack = nn.Sequential(\n            nn.Conv2d(3, 16, 3),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, 3),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 3),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Flatten(),\n            nn.Linear(10816, out_features=10)\n        )\n        \n\n    def forward(self, x):\n        # print(x.shape)\n        # x = self.flatten(x)\n        # print(x.shape)\n        #logits = self.linear_relu_stack(x)\n        y = self.Conv2d_stack(x)\n        return y\n\nmodel = NeuralNetwork().to(device)\n# for X, y in test_dataloader:\n#     output = model(X)\n#     break\nprint(model)","metadata":{"id":"hriAYz8zF8gp","outputId":"2f9780a5-d121-4334-d34e-1e66c7e993e1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# SGD Optimizer\n#optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-4)\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-4, momentum=0.9)\n#scheduler = ExponentialLR(optimizer, gamma=0.9)","metadata":{"id":"0leI9lpyF8gs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training function\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    \n    # Turn on training mode\n    model.train()\n    train_loss, correct = 0, 0\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        # Compute prediction error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # record loss\n        train_loss += loss.item()\n        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    #scheduler.step()\n    \n    train_loss /= len(dataloader)\n    correct /= size\n    \n    print(f\" Train accuracy: {(100*correct):>0.1f}%, Avg loss: {train_loss:>8f}\")   \n    trainLoss_plot.append(train_loss)\n    trainAcc_plot.append(100*correct)","metadata":{"id":"1mwypk0vF8gv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test function\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    \n    # Turn on evalution mode\n    model.eval()\n    test_loss, correct = 0, 0\n    \n    # Turn off gradient descent\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            \n            # record loss\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n            \n    test_loss /= num_batches\n    correct /= size\n    \n    print(f\" Test accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n    testLoss_plot.append(test_loss)\n    testAcc_plot.append(100*correct)","metadata":{"id":"FnotisWFF8gx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Total training epochs\nepochs = 100\n\nfor t in range(epochs):\n    print('\\n', \"=\" * 15, \"Epoch\", t + 1, \"=\" * 15)\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\n    \nprint(\" Done!\")","metadata":{"id":"7UHb6IY3F8gz","outputId":"11daa936-9195-4858-a019-4b1bf7e3f8b7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(trainLoss_plot)\nplt.plot(testLoss_plot)\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n","metadata":{"id":"_PRGPq0utb-f","outputId":"fb189e17-642a-48ea-9456-558b31b63a3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(trainAcc_plot)\nplt.plot(testAcc_plot)\nplt.title('model accuracy')\nplt.ylabel('accuracy(%)')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"id":"A5KGmeoAebTj","outputId":"00ec4175-c904-4e70-d143-b882a1c80791"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving model weights\n# torch.save(model)\ntorch.save(model.state_dict(), \"model.pth\")\nprint(\" Saved PyTorch Model State to model.pth\")","metadata":{"id":"2o5SZctiF8g1","outputId":"98a91774-3f68-41e5-8b69-a0e9afb14e8f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build the network\nmodel = NeuralNetwork()\n\n# Load trained weights\nmodel.load_state_dict(torch.load(\"model.pth\"))","metadata":{"id":"of7Rn3FNF8g2","outputId":"1d77de4b-2b24-4f84-bdcc-c7f2f62e31a8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 10 Classes\nclasses = [\n    \"Airplane\",\n    \"Automobile\",\n    \"Bird\",\n    \"Cat\",\n    \"Deer\",\n    \"Dog\",\n    \"Frog\",\n    \"Horse\",\n    \"Ship\",\n    \"Truck\",\n]\n\n# Evaluation mode\nmodel.eval()\n\n# Get one sample\nx, y = torch.tensor(test_data.data[10]).float().unsqueeze(0), test_data.targets[10]\n# print(x.shape)\n# x, y = test_data.data[0], test_data.targets[1]\n\n# Turn off gradient descent\nwith torch.no_grad():\n    pred = model(x)\n    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n    print(f' Predicted: \"{predicted}\", Actual: \"{actual}\"')","metadata":{"id":"wkczlBqQF8g3","outputId":"835dcf8a-8d33-48a6-8425-afa5e58cd53a"},"execution_count":null,"outputs":[]}]}